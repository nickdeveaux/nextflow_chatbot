# Nextflow Chat Assistant

**ğŸŒ [Live Demo](https://nextflow-chatbot-git-main-nicholas-de-veauxs-projects.vercel.app?_vercel_share=DTofc9K3c2uHZqB3mInf0dJsfeWEBqcj)**

A chat assistant that answers Nextflow documentation questions using semantic search and LLM responses.

## Features

- **Multi-turn conversations** - Maintains context within sessions
- **Vector search** - Semantic search over Nextflow documentation with citations
- **Responsive UI** - Works on desktop and mobile devices (iOS-friendly)
- **Markdown rendering** - Rich text formatting for code blocks, links, lists
- **Error handling** - Graceful degradation with clear error messages
- **Loading states** - Animated loading indicators
- **Dark mode** - Theme toggle for user preference
- **Character limit** - Input validation with visual feedback
- **Security** - Light prompt injection detection

## Tech Stack

- **Frontend**: Next.js 14 with TypeScript, React, Markdown rendering
- **Backend**: FastAPI (Python) with async support
- **LLM**: Google Gemini 2.0 Flash via Vertex AI
- **Vector Store**: FAISS with sentence-transformers for semantic search
- **Deployment**: Railway (backend) + Vercel (frontend)

## Local Setup

See [Quick Start Guide](docs/quick-start.md) for local development setup instructions.

## Deployment

See [Deployment Guide](docs/deployment.md) for detailed deployment instructions.

## Author's Notes

### Process:

These **Author's Notes** are a chance for me to write unfiltered by compilers or llms!

Claude Code and Cursor were used extensively for the first half of the project (green field), but when it came to brown-field improvements, deployment optimization and troubleshooting, and the llm libraries, I had to work on those by hand. 

I chose python logic and sentence-transformers because I have experience with both of those libraries. I've only used typescript here and there before, but I like it, and wanted to try it -- however the frontend is where I relied the most on Claude. 

### Abandoned paths:

I had a scraper and started scraping the discussions on GitHub and Seqeraâ€™s site, in order to grow the embedded document corpus, but I came across a few roadblocks: it wasnâ€™t immediately clear whether the Q&A was still relevant (on a recent version), whether the answer would be good enough to want to cite, and sometimes the question included stack traces that made it very long and not easily embeddable for vector search. So instead I tried to summarize as much as I could from those discussions into the main prompt.

I also previously had a check on GPU to see which version of torch to install, but once I was able to fit the CPU-version into Railway, I realized I could make the code simpler by making it always use the most minimal requirements doc (not having to download Nvidia libs). 

Also, since I used Railway and local development, I removed my docker-compose.yml file to have one less thing to maintain. 

### Compromises:

I was at first happy to have been able to put all my config in a .yaml, but then I realized Vercel only uses code in the front end directory, so I think in retrospect it might have been simpler to repeat my front-end config instead of trying to port it over from the yaml with config-porting scripts. 

I used Googleâ€™s LLM because I have a 90 day free trial with Google Cloud. In retrospect, it might have been interesting to test others, but my instinct is that it wonâ€™t make too much of a difference. In order to now become too enmeshed with it, I implemented conversation history tracking without having to rely on it to give me a conversation ID, which makes the gen-ai library more replaceable. 


## Project Structure

```
backend/
â”œâ”€â”€ main.py                    # FastAPI app entry point
â”œâ”€â”€ config.py                  # Configuration loader
â”œâ”€â”€ models.py                  # Pydantic models
â”œâ”€â”€ llm_client.py              # LLM client wrapper
â”œâ”€â”€ llm_utils.py               # LLM utilities
â”œâ”€â”€ session_manager.py         # Session management
â”œâ”€â”€ security.py                # Security checks
â”œâ”€â”€ context_formatter.py       # Context formatting
â”œâ”€â”€ vector_store_manager.py    # Vector store initialization
â”œâ”€â”€ citations.py               # Citation extraction
â”œâ”€â”€ logging_config.py          # Logging setup
â”œâ”€â”€ vector_store/              # Vector store implementation
â”œâ”€â”€ requirements.txt           # Dependencies
â”œâ”€â”€ Dockerfile                 # Backend Dockerfile
â”œâ”€â”€ test_*.py                  # Unit tests
â””â”€â”€ integration_test_chat.py   # Integration test for chat endpoint

frontend/
â”œâ”€â”€ app/                       # Next.js app directory
â”‚   â”œâ”€â”€ page.tsx               # Main page (composition container)
â”‚   â”œâ”€â”€ layout.tsx             # Root layout
â”‚   â”œâ”€â”€ globals.css            # Global styles
â”‚   â”œâ”€â”€ types.ts               # TypeScript interfaces
â”‚   â”œâ”€â”€ utils.ts               # Utility functions
â”‚   â”œâ”€â”€ hooks/                 # Custom React hooks
â”‚   â”‚   â”œâ”€â”€ useChat.ts         # Chat state and message handling
â”‚   â”‚   â”œâ”€â”€ useDarkMode.ts     # Dark mode management
â”‚   â”‚   â”œâ”€â”€ useBackendHealth.ts # Backend health monitoring
â”‚   â”‚   â”œâ”€â”€ useInput.ts        # Input state and validation
â”‚   â”‚   â””â”€â”€ useLoadingMessage.ts # Loading message rotation
â”‚   â””â”€â”€ components/            # UI components
â”‚       â”œâ”€â”€ ChatHeader.tsx     # Header with theme toggle
â”‚       â”œâ”€â”€ ChatMessages.tsx   # Messages container
â”‚       â”œâ”€â”€ ChatMessage.tsx    # Individual message component
â”‚       â”œâ”€â”€ ChatInput.tsx      # Input area with character counter
â”‚       â”œâ”€â”€ EmptyState.tsx     # Welcome/empty state
â”‚       â”œâ”€â”€ ErrorMessage.tsx   # Error display
â”‚       â”œâ”€â”€ LoadingIndicator.tsx # Loading state
â”‚       â”œâ”€â”€ BackendUnavailableModal.tsx # Backend unavailable modal
â”‚       â””â”€â”€ markdownComponents.tsx # Markdown renderer components
â”œâ”€â”€ config.ts                  # Frontend config (auto-generated)
â”œâ”€â”€ package.json               # Dependencies
â”œâ”€â”€ Dockerfile                 # Frontend Dockerfile
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ sync-config-standalone.js  # Config sync (Vercel)
â””â”€â”€ __tests__/                 # Frontend tests

scripts/
â””â”€â”€ sync-config.js             # Config sync (local/dev)

docs/                          # Documentation
â”œâ”€â”€ architecture.md            # System architecture
â”œâ”€â”€ deployment.md              # Deployment guide
â”œâ”€â”€ quick-start.md             # Local development setup
â”œâ”€â”€ vector-store.md            # Vector store documentation
â””â”€â”€ ...                        # Other documentation

config.yaml                    # Shared configuration
README.md                      # Main documentation
```

See [Architecture Documentation](docs/architecture.md) for system architecture and component overview.

## Testing

### Backend Tests

**Unit tests:**
```bash
cd backend
pytest test_*.py -v  # Run all tests
# Or run specific test files:
pytest test_main.py test_citations.py test_config.py test_vector_store.py test_llm_client.py -v
```

**Integration test:**
```bash
cd backend
# Make sure the backend is running on http://localhost:8000
python integration_test_chat.py
```

This integration test:
- Tests the chat endpoint with sample questions
- Verifies session management (multi-turn conversations)
- Checks for citations in responses
- Validates backend health

### Frontend Tests

Run frontend tests: `cd frontend && npm test`

## License

MIT

