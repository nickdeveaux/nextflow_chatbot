[
  {
    "text": "It is possible to use `publishDir` in a process, but it doesn't seem possible to publish a workflow output channel (DSL2).",
    "metadata": {
      "platform": "github",
      "source": "GitHub Discussions",
      "url": "https://github.com/nextflow-io/nextflow/discussions/1933",
      "title": "How to use `publishDir` on a workflow output?",
      "created_at": "2020-06-17T10:17:34Z"
    }
  },
  {
    "text": "Hello! I'm ramping up with DSL2, and hit a roadblock dealing with processes outputting a glob of files. I wonder if there is a better way to deal with these lists of files, but hopefully my code may help others.\r\n\r\nDSL2 modules are great, but the `publishDir` directive is unwieldy for processes imported across multiple workflows; I've found it more tenable to explicitly define a tree of results at the end of my workflow. This requires mapping single files, files in directories, and array lists globbed files. I've illustrated this below. \r\n\r\nI've been confused by the changes to:\r\n- paths (which may be a single file, a glob or array list of files, or a directory - each with their own behavior)\r\n- files (`file()` appears to be supported, but the type is not recommended in process outputs in DSL2)\r\n- channels, which mostly seem to have fallen out of favor in DSL2 processes, and I haven't had much need for, except apparently in dealing with file globs, as below.\r\n\r\nUltimately, I'm wondering if there is a better way to define `copyFilesFromGlob` - and think the docs would benefit from some further explanation or examples dealing with array lists of file paths:\r\n\r\n```\r\nnextflow.enable.dsl=2\r\n\r\nprocess create_files {\r\n\toutput:\r\n\t\tpath '0.txt', emit: \"zero\"\r\n\t\tpath '*.txt', emit: \"files\"\r\n\t\tpath 'dir',   emit: \"dir\", type: \"dir\"\r\n\tscript:\r\n\t\t'''\r\n\t\techo 'zero' > 0.txt\r\n\t\techo 'one' > 1.txt\r\n\t\techo 'two' > 2.txt\r\n\r\n\t\tmkdir -p dir\r\n\t\techo 'three' > dir/3.txt\r\n\t\techo 'four' > dir/4.txt\r\n\t\t'''\r\n}\r\n\r\ndef copyFilesFromGlob(files, target) {\r\n\t// files is an array of paths\r\n\t// appears to require an unintuitive subscribe() - e.g. each() does not work\r\n\tfiles.collect().flatten().subscribe { p -> \r\n\t\tf = file(p)\r\n\t\tif ( f.isFile() ) { \r\n\t\t\tf.copyTo(\"${target}/${f.getName()}\")\r\n\t\t}\r\n\t}\r\n}\r\n\r\ndef copyDirectoryFiles(dir, target) {\r\n\tdir.eachFile { f -> \r\n \t\tif ( f.isFile() ) { \r\n \t\t\tf.copyTo(\"${target}/${f.getName()}\")\r\n \t\t}\r\n \t}\r\n}\r\n\r\nworkflow {\r\n\tmain:\r\n\t\tcreate_files()\r\n\t\t(zero, files, dir) = create_files.out\r\n\r\n\t\t// creates /tmp/0.txt\r\n\t\t// [expected behavior]\r\n\t\tzero.copyTo(\"/tmp/0.txt\")\r\n\r\n\t\t// fails\r\n\t\t// [expected, but docs not clear on what to do instead]\r\n\t\t// files.copyTo(\"/tmp\")\r\n\r\n\t\t// fails\r\n\t\t// [unexpected, and not obvious to debug]\r\n\t\t// files.eachFile(f -> f.copyTo(\"/tmp/${f.getName()}\"))\r\n\r\n\t\t// fails\r\n\t\t// [was surprising to me that files.eachFile() does not work]\r\n\t\t// copyDirectoryFiles(files, \"/tmp\")\r\n\r\n\t\t// creates /tmp/{1,2}.txt\r\n\t\t// [was not obvious to me in the docs how to achieve this behavior]\r\n\t\tcopyFilesFromGlob(files, \"/tmp\")\r\n\r\n\t\t// creates /tmp/{3,4}.txt\r\n\t\t// [expected]\r\n\t\tcopyDirectoryFiles(dir, \"/tmp\")\r\n\r\n\t\t// creates /tmp/dir/{3,4}.txt\r\n\t\t// [reasonable, but I had expected the files to copy to /tmp directly]\r\n\t\t// dir.copyTo(\"/tmp\") \r\n}\r\n```\r\n\r\nThanks for any pointers!",
    "metadata": {
      "platform": "github",
      "source": "GitHub Discussions",
      "url": "https://github.com/nextflow-io/nextflow/discussions/1920",
      "title": "Dealing with file globs in DSL2",
      "created_at": "2020-08-25T02:35:19Z"
    }
  },
  {
    "text": "Hello,\r\nI've been trying to learn how to migrate my workflows from nextflow DSL1 to DSL2, and I got a question concerning the use of sub-workflows. \r\nI'm trying to migrate a small workflow that convert two genomes in 2bit format, and then generates the sizes files from those. Each of the two is a separate small workflow (creation of 2bit, followed by generation of the sizes), and the output of the first should be passed to the second.\r\nI've created a structure of folders as follow:\r\n```\r\n./\r\n./modules/\r\n./modules/subworkflow\r\n./modules/processes\r\n./main.nf\r\n```\r\nWith the ```subworkflow``` contiaining the sub-workflow in .nf format and ```processes``` containing the processed that can be included by the different functions.\r\nI got a main workflow as follow:\r\n```\r\ninclude {TWOBIT; SIZES} from './modules/subworkflows/preprocess' params(params)\r\nworkflow {\r\n        main:\r\n                TWOBIT()\r\n                TBS = TWOBIT.out.twoBitS\r\n                TBT = TWOBIT.out.twoBitT\r\n                SIZES(TBS, TBT)\r\n}\r\n```\r\nThis is calling these two sub-workflows, that look as follow:\r\n```\r\ninclude {make2bitS; make2bitT; makeSizeS; makeSizeT} from \"../processes/preprocess\" params(params)\r\n\r\nworkflow TWOBIT {\r\n    main:\r\n        // Make 2bit genomes\r\n        make2bitS()\r\n        make2bitT()\r\n    emit:\r\n        twoBitS = make2bitS.out.twoBsrc\r\n        twoBitT = make2bitT.out.twoBtgt\r\n\r\n}\r\n\r\nworkflow SIZES {\r\n    take:\r\n        2bitS\r\n        2bitT\r\n    main:\r\n        makeSizeS(2bitS)\r\n        makeSizeT(2bitT)\r\n    emit:\r\n        twoBitS = makeSizeS.out.sizesSrc\r\n        twoBitT = makeSizeT.out.sizesTgt\r\n\r\n}\r\n```\r\nThese in turn call the following functions:\r\n```\r\nprocess make2bitS {\r\n    tag \"twoBit\"\r\n    publishDir \"$params.outdir/genome2bit\"\r\n\r\n    output:\r\n    path \"source.2bit\", emit: twoBsrc\r\n\r\n    script:\r\n    \"\"\"\r\n    faToTwoBit ${params.source} source.2bit\r\n    \"\"\"\r\n}\r\n\r\nprocess makeSizeS {\r\n    tag \"twoBit\"\r\n    publishDir \"$params.outdir/genome2bit\"\r\n    input:\r\n    path src\r\n    output:\r\n    path \"source.sizes\", emit: sizesSrc\r\n    script:\r\n    \"\"\"\r\n    twoBitInfo ${src} source.sizes\r\n    \"\"\"\r\n}\r\nprocess make2bitT {\r\n    tag \"twoBit\"\r\n    publishDir \"$params.outdir/genome2bit\"\r\n    output:\r\n    path \"target.2bit\", emit: twoBtgt\r\n    script:\r\n    \"\"\"\r\n    faToTwoBit ${params.target} target.2bit\r\n    \"\"\"\r\n}\r\nprocess makeSizeT {\r\n    tag \"twoBit\"\r\n    publishDir \"$params.outdir/genome2bit\"\r\n    input:\r\n    path tgt\r\n    output:\r\n    path \"target.sizes\", emit: sizesTgt\r\n    script:\r\n    \"\"\"\r\n    twoBitInfo ${tgt} target.sizes\r\n    \"\"\"\r\n}\r\n``` \r\n\r\nHowever, when I run the pipeline, I get the following error:\r\n```\r\nModule compilation error\r\n- file : ./modules/subworkflows/preprocess.nf\r\n- cause: Workflow malformed parameter definition @ line 32, column 9.\r\n           2bitS\r\n           ^\r\n\r\nModule compilation error\r\n- file : ./modules/subworkflows/preprocess.nf\r\n- cause: Workflow malformed parameter definition @ line 33, column 9.\r\n           2bitT\r\n           ^\r\n\r\n2 errors\r\n```  \r\n\r\nSorry, it's probably a very simple mistake, but I'm not sure to understand what it is it doesn't like. \r\n\r\nThank you in advance\r\nAndrea",
    "metadata": {
      "platform": "github",
      "source": "GitHub Discussions",
      "url": "https://github.com/nextflow-io/nextflow/discussions/1918",
      "title": "Connecting subworkflows with DSL2",
      "created_at": "2020-11-16T17:07:56Z"
    }
  },
  {
    "text": "## question_description\r\n- my platform is torque;\r\n- but the HPC use dsub to submite jobs, so i cant submite jobs use nextflow default qsub;\r\n- and also, there is a line in submitting jobs, like below, which is not print by nextflow torque api;\r\n```\r\n#PBS -j oe\r\n#HSCHED -s hschedd\r\n```\r\n## my solution\r\n- write a submite script and apply resources, and put nextflow run in the script, config nextflow to processor.local\r\n- will this method use HPC resources corretly ?\r\n\r\n## can I change some where to use dsub and add line `#HSCHED -s hschedd` in PBS header;",
    "metadata": {
      "platform": "github",
      "source": "GitHub Discussions",
      "url": "https://github.com/nextflow-io/nextflow/discussions/1921",
      "title": "Modify cluster options for PBS via dsub",
      "created_at": "2020-12-09T03:26:17Z"
    }
  },
  {
    "text": "Hello, we need to run nextflow platform under Java + Spring context, not using CLI.\r\n\r\nCan you describe how we can run it?)",
    "metadata": {
      "platform": "github",
      "source": "GitHub Discussions",
      "url": "https://github.com/nextflow-io/nextflow/discussions/1917",
      "title": "Run NextFlow as JAR library",
      "created_at": "2020-12-16T11:13:32Z"
    }
  },
  {
    "text": "Is it possible to defined `workflow.onComplete` and `workflow.onError` on a named workflow?\r\n\r\nMy use-case is a main workflow with a switch statement that calls nested workflows. I'd like to define the handlers on the nested workflows, i.e. the named ones.\r\n\r\nSomething like:\r\n```\r\nworkflow my_nested_workflow {\r\n    ...\r\n}\r\n\r\nmy_nested_workflow.onComplete {\r\n    ...\r\n}\r\n\r\n```",
    "metadata": {
      "platform": "github",
      "source": "GitHub Discussions",
      "url": "https://github.com/nextflow-io/nextflow/discussions/1919",
      "title": "Possible to call workflow handler in DSL2 on nested workflow?",
      "created_at": "2021-01-07T18:47:29Z"
    }
  },
  {
    "text": "Hi everyone,\r\n\r\nI'm wondering whether it's worth adding a `.directory.tree` (or a better named file) which contains the output of `tree` command when the task exits.\r\n\r\nOften times while debugging, we need to inspect the contents of the directory and having a dedicated file would help automate the process. We could add a separate `Directory contents` tab in `Tower` for every process window as well.\r\n\r\nPlease let me know if this proposal makes sense or not.\r\n\r\nBest,\r\nAbhinav",
    "metadata": {
      "platform": "github",
      "source": "GitHub Discussions",
      "url": "https://github.com/nextflow-io/nextflow/discussions/1877",
      "title": "Are the contents of a task work directory worth adding to the task level logs?",
      "created_at": "2021-02-02T17:12:21Z"
    }
  },
  {
    "text": "Hi team,\r\n\r\nI was wondering whether we could simplify the process of helping users to test their code against the latest `stable` and `edge` version.\r\n\r\nThe proposal is to add two special labels `stable` and `edge` which would automatically resolve to the latest stable and edge release from Github.\r\n\r\nFor example,\r\n```\r\nNXF_VER=edge nextflow run nextflow-io/hello\r\n```\r\n\r\nI think that this could be an additive change, which retains the current ability to specify the exact version which the user would like to use.",
    "metadata": {
      "platform": "github",
      "source": "GitHub Discussions",
      "url": "https://github.com/nextflow-io/nextflow/discussions/1892",
      "title": "Adding special labels for the NXF_VER env variable",
      "created_at": "2021-02-15T15:23:13Z"
    }
  },
  {
    "text": "Hi team,\r\n\r\nas there was noone taking up the baton from this Thread here https://twitter.com/tallphil/status/1360938099925213184 I tried  to give it a start and maybe others can chime in. \r\n\r\nnf-core recently started with work mostly done by @KevinMenden , @drpatelh , @ewels and others to have validation of input parameters for pipelines. They created a way to validate parameters supplied to a pipeline against the already established JSON schema that nf-core pipelines inherently use for creating the launch feature on the webpage. \r\n\r\nAn example including it into the latest nf-core template on the `dev` branch can be found here:\r\n\r\nhttps://github.com/nf-core/tools/pull/852\r\n\r\nPaolo suggested that this could be something that Nextflow itself could potentially support as a core feature. There are several points that this currently does:\r\n\r\n- Validation of supplied parameters against the ones specified in the JSON schema\r\n- Raising warnings if users specify parameters that are NOT in the JSON schema\r\n- Raising warnings if users specify parameters incorrectly that are core nextflow parameters (e.g. having a `--profile` , though I'm not sure what happens with a `--profile foobar -profile test,docker` ðŸ¤” )\r\n- Generating automatic help & summary pages when users specify `--help` from the JSON schema (=> no need to keep this manually in sync, much less hardcoded parts in the pipeline & works generically between pipelines)\r\n\r\nWe also have further functionality in there, but these seem to be rather nf-core specific (e.g. the nf-core headers, e-mail feature are also in separate `lib/` code pieces), so maybe not as suitable as the above to be of potential interest for core nextflow features. \r\n\r\nJust wanted to give this a kick off and bring in the people really involved, I was just a fortunate front-line & happy prototype user ðŸ¤—",
    "metadata": {
      "platform": "github",
      "source": "GitHub Discussions",
      "url": "https://github.com/nextflow-io/nextflow/discussions/1893",
      "title": "Parameter Validation",
      "created_at": "2021-02-15T15:49:10Z"
    }
  },
  {
    "text": "Inputs within a process should be able to run in the ordered they are received. Currently documentation states, 'The channel guarantees that items are delivered in the same order as they have been sent - but - since the process is executed in a parallel manner, there is no guarantee that they are processed in the same order as they are received.'\r\n\r\nIt would be beneficial to be able to process inputs in the order they are received so that inputs that will take longer can be started before shorter inputs. This will have an impact on the overall run time of the pipeline.\r\n\r\n## Usage scenario \r\n\r\nThis would be beneficiialy when running a process in parallel per chromosome. Larger chromosomes (ie 1-4) will take longer than smaller chromosomes (ie 20-22). You would want to be able to give an ordered list to the process of which chromosomes (inputs) to run first. If there are only enough resources to run x number of inputs in parallel than you would want to start the longer jobs first.",
    "metadata": {
      "platform": "github",
      "source": "GitHub Discussions",
      "url": "https://github.com/nextflow-io/nextflow/discussions/1901",
      "title": "Ability to run parallel process in a specific order",
      "created_at": "2021-02-16T08:34:25Z"
    }
  },
  {
    "text": "Hello,\r\n\r\nit would be useful to have a finer control over the nextflow behavior when comes to resuming operation. Specifically, I would like to be able to choose which steps should be recreated and which should be left as is. This is motivated by the size of the data some of the pipelines I work on process (e.g. 160 TB). The current nextflow strategy to redo everything if anything changes, be it the code or the input data, is safe and guarantees the integrity of the results. However, oftentimes the user is better positioned to make a decision what really needs to be rerun and how costly the alternative of rerunning everything is.\r\n\r\nI would find useful to have something like:\r\n```\r\n# when resuming the operation, don't redo any files\r\nnextflow run -resume -no-redo\r\n\r\n# when resuming the operation, redo only files generated by step3 and subsequent\r\nnextflow run -resume -redo step3\r\n```\r\n\r\nAlso it would be helpful to be able to specify in the workflow which steps are temporary and should be cleaned from the hard drive after a specified process finishes. I've seen some solutions to do this (e.g. https://github.com/wtsi-hgi/nextflow_ci/blob/template/pipelines/main.nf#L54-L78), but they seemed to be working around nextflow limitations, rather than an integral solution. Ideally one would say this very simply, something like\r\n```\r\n# clean all directories generated by channel1 after channel2 is completed\r\nchannel1.clean_on_completion(channel2)\r\n```",
    "metadata": {
      "platform": "github",
      "source": "GitHub Discussions",
      "url": "https://github.com/nextflow-io/nextflow/discussions/1897",
      "title": "Finer control of -resume, cached results, and cleaning of temporary results",
      "created_at": "2021-02-17T10:43:10Z"
    }
  },
  {
    "text": "Hi, I am a new user of nextflow. I have test1.bed, test1.bim, test1.fam files. I want to do some qc using plink in nextflow. How can I import those three files in nextflow and get output test2.bed, test2.bim, test2.fam which will be used in the next step to generate test3.bed, test3.bim, test3.fam and so on? I also need to save all the intermediate files in a directory. Any help with example?\r\nKind regards, Zillur",
    "metadata": {
      "platform": "github",
      "source": "GitHub Discussions",
      "url": "https://github.com/nextflow-io/nextflow/discussions/1900",
      "title": "Pipeline with multiple input and multiple output",
      "created_at": "2021-02-18T09:35:36Z"
    }
  },
  {
    "text": "Hi, I am referring to this section: https://www.nextflow.io/docs/latest/google.html#preemptible-instances\r\n\r\nYes it is true, that if a VM gets preemted, the error code is 14, but that is not the whole truth.\r\n\r\nAccording to the Google Life Sciences documentation it can be 8, 10 or 14. See: https://cloud.google.com/life-sciences/docs/troubleshooting, Section: Retrying after encountering errors\r\n\r\nIt would be nice to add that info to the docs as well.",
    "metadata": {
      "platform": "github",
      "source": "GitHub Discussions",
      "url": "https://github.com/nextflow-io/nextflow/discussions/1913",
      "title": "Accomodating GCP error codes in docs and workflow design.",
      "created_at": "2021-02-22T09:01:41Z"
    }
  },
  {
    "text": "Hey, I have the following situation: I need to preprocess a few files before the actual analysis can happen and I have two sets of them:\r\n\r\n- calibration files\r\n- actual data files\r\n\r\nThere are just a few calibration files and each data files requires the right one. To pick the correct calibration, one needs to inspect the file.\r\n\r\nThis is how my workflow looks like for now, to demonstrate the situation, given the following files:\r\n\r\n- calibrations: `A.detx`, `B.detx`, `C.detx`\r\n- data files: `1.evt.gz`, `2.evt.gz`, `3.evt.gz`, `4.evt.gz`, `5.evt.gz`, `6.evt.gz`, `7.evt.gz`\r\n\r\n```\r\nworkflow {\r\n  data = channel.fromPath('data/*.evt.gz')\r\n  calibrations = channel.fromPath('detectors/*.detector')\r\n\r\n  preprocess_data(data)\r\n  preprocess_calibrations(calibrations)\r\n\r\n  combined = ???  // create channel with combined data/calibration\r\n\r\n  // actual_analysis(combined)\r\n}\r\n```\r\n\r\nI need some mechanism/process (`combined` in the above workflow) which takes the preprocessed `data` and `detectors` and then creates a list of tuples out of them, like: `[(B.detx, 1.evt.gz), (A.detx, 2.evt.gz), (A.detx, 3.evt.gz), (A.detx, 4.evt.gz), (A.detx, 5.evt.gz), (A.detx, 6.evt.gz), (C.detx, 7.evt.gz)]`. In this `combined` process, I need access to all the `data` and `calibration` files to be able to emit a corresponding pair. As written above, to pick the correct calibration, I need to do some quick checks on the `data` files...\r\n\r\nI checked the operators but could not figure out which one could fit. I also checked the patterns in https://github.com/nextflow-io/patterns/tree/master/docs but they seem to be a bit outdated and I anyways need DSL2.",
    "metadata": {
      "platform": "github",
      "source": "GitHub Discussions",
      "url": "https://github.com/nextflow-io/nextflow/discussions/1922",
      "title": "Combining two channels and emitting tuples with repeated elements",
      "created_at": "2021-02-23T15:20:21Z"
    }
  },
  {
    "text": "I have a sequence database (~ 500 gb) deposited in a Google bucket that is used by some search process. When I execute this workflow, it spends 2 hours transferring data into the workflow execution directory. Is there a way to just mount the database bucket using `gsutils`?\r\n\r\nThank you!",
    "metadata": {
      "platform": "github",
      "source": "GitHub Discussions",
      "url": "https://github.com/nextflow-io/nextflow/discussions/1947",
      "title": "Mount persistent storage w/ e.g. databases into process",
      "created_at": "2021-03-01T05:11:11Z"
    }
  },
  {
    "text": "We are looking to make the main nextflow run more memory efficient by identifying memory leaks and objects that require a lot of the heap. However we are unsure of where to start and have not found a lot of guidance on how to best address it specifically for nextflow. We are processing >10,000 and at one process, a sample will spawn about 60 jobs for variant calling different parts of the genome. this means our runs are submitting a lot of jobs and we also have a large amount of channels that don't close because we are using `watchPath` on up to 4 files at a time. the maps are few, quite small and updated infrequently. \r\n\r\nWe are already fine-tuning `NXF_OPTS` and heap size variables, but this seems to just be postponing the problem to another day instead of addressing underlying issues in the workflow. any guidance would be appreciated. \r\n\r\nfinally i do want to mention that i have a TimerTask in my workflow:\r\n```\r\ndef touchInputs() {\r\n  new Timer().schedule({\r\n  for ( i in epochMap.keySet() ){\r\n    fileEpoch = file(i).lastModified()\r\n    if ( fileEpoch > epochMap[i]) {\r\n      epochMap[i] = fileEpoch\r\n      \"touch -ca ${i}\".execute()\r\n    }\r\n  }\r\n} as TimerTask, 1000, params.touchInputsInterval * 60 * 1000 ) // convert minutes to milliseconds\r\n}\r\n```\r\nthe map operated on in the above snippet has only four keys in it and i do not believe it to be a memory leak, but maybe someone else would be able to identify something. `touchInputs()` is only run once.",
    "metadata": {
      "platform": "github",
      "source": "GitHub Discussions",
      "url": "https://github.com/nextflow-io/nextflow/discussions/1944",
      "title": "Identifying memory leaks in nextflow workflow",
      "created_at": "2021-03-02T00:16:25Z"
    }
  },
  {
    "text": "Hi Nextflow community members,\r\n\r\nFollowing the suggestions by @pditommaso and @abhi18av, I am glad to use this _**Show and tell**_ section to share with you all a tool we recently developed to support workflow code reuse via shareable packages. The tool is called WorkFlow Package Manager (WFPM) CLI. Of course, it's open source: https://github.com/icgc-argo/wfpm. Feel good to have something share back to the open source community. Needless to say, without the awesome Nextflow framework, nothing about WFPM would be possible.\r\n\r\nAs a key concept in WFPM, a package is the smallest unit of shareable workflow code bundle. Concretely, a package can include one or more of these items: process, function or workflow. Each package must contain a _**pkg.json**_ file to keep basic information, such as package name, version, main entry point, source code repository, container registry, dependencies etc.\r\n\r\nThe WFPM CLI development aims to make it as easy as possible to collaboratively develop [ICGC ARGO](https://www.icgc-argo.org/) bioinformatics workflows that conform to ARGO's established best practice guidelines: reproducible, portable, composable, findable and testable.\r\n\r\nNoticeable features supported by WFPM CLI:\r\n* auto-generated templates including workflow code and testing code\r\n* seamless integration with GitHub Actions for automated CI/CD (support for other platform like GitLab is under consideration)\r\n* everything developed using WFPM CLI is a package\r\n* every package is independently developed and published with strict release versioning\r\n* every package must include tests that will be automatically invoked by CI/CD processes\r\n* published package is immutable and permanently avaialble\r\n* anyone can write and publish a package\r\n* a published package can be imported by anyone into her own workflow code base\r\n* package import must explicitly specify a particular version to be imported to ensure reproducibility, this is similar to the idea of the _package-lock.json_ file in NPM to ensure reproducible builds\r\n* a package can import other packages (which are also known as dependencies)\r\n* the above feature enables the support for subworkflows, ie, importing a published workflow as a subworkflow to build larger workflows\r\n\r\nCode reuse in many general purpose programming languages is natively supported and often taken for granted, however it's not easy to do in workflow development. For example, you can do `import yaml` in Python to bring in all the great functionalities supported by the package which you rarely know or need to know who developed it. With features offered by WFPM, similar code import in workflow development becomes possible and easy to do using reusable and shareable packages.\r\n\r\nThe WFPM CLI development is at early stage, but we feel it has sufficient features for users of the amazing Nextflow community to try it out. As we have been mainly focusing on ARGO's primary needs, undoubtedly, there are weak areas we need to improvement on. We'd love to hear feedback and suggestions from you, what you like or dislike, all is greatly appreciated!",
    "metadata": {
      "platform": "github",
      "source": "GitHub Discussions",
      "url": "https://github.com/nextflow-io/nextflow/discussions/1942",
      "title": "Introducing WorkFlow Package Manager (WFPM) CLI",
      "created_at": "2021-03-02T00:38:28Z"
    }
  },
  {
    "text": "Howdy!\r\n\r\nIt's been a few months, some good, some bad, but all filled with great nextflow progress!\r\n\r\nI couldn't find a documented release schedule or process. Is this documented anywhere?\r\n\r\nEven if it's just \"I release when I have time\" - might be worth adding to the README. There's some unreleased features that would be helpful to have.\r\n\r\nI do know about the `-edge` releases. It's also not clear _how_ edge-y those are. Any guidance would be appreciated. If there's already words on this, feel free to point me at docs.",
    "metadata": {
      "platform": "github",
      "source": "GitHub Discussions",
      "url": "https://github.com/nextflow-io/nextflow/discussions/1945",
      "title": "Thoughts on documenting a release process/schedule",
      "created_at": "2021-03-02T02:45:53Z"
    }
  },
  {
    "text": "## Bug report \r\nworkflow {\r\n     HELLO()\r\n     HELLO()\r\n}\r\n### Expected behavior and actual behavior\r\nexpected behavior: Run process HELLO twice\r\nactual result : Process 'HELLO' has been already used -- If you need to reuse the same component include it with a different name or include in a different workflow context\r\n\r\n* Nextflow version: 20.10.0 \r\n* Java version: 8\r\n* Operating system: [Ubuntu 18.04]\r\n* Bash version: 5.0.17(1)-release (x86_64-pc-linux-gnu)\r\n\r\n### Additional context\r\n\r\nDSL 2 is enabled.\r\n\r\nis I have any way to run the processes on loop?\r\nsomething like:\r\n    workflow {\r\n     for (int i=0; I< 5; i++){\r\n           HELLO()\r\n     }\r\n}",
    "metadata": {
      "platform": "github",
      "source": "GitHub Discussions",
      "url": "https://github.com/nextflow-io/nextflow/discussions/1949",
      "title": "How to reuse same process within a workflow?",
      "created_at": "2021-03-02T19:12:49Z"
    }
  },
  {
    "text": "Hello,\r\n\r\nI am reading through the Nextflow documentation. I am confused by [something](https://www.nextflow.io/docs/latest/process.html#input-of-files) I read in Processes/Inputs/Input of files. In the example, `proteins` is defined in terms of a file path:\r\n`proteins = Channel.fromPath( '/some/path/*.fa' )`.\r\nBut the tip at the bottom says, \"This is also the reason why you should avoid whenever possible to use absolute or relative paths referencing files in your pipeline processes.\" So I have two questions.\r\n\r\n1. When should I be using file paths? At the beginning of the script and never again?\r\n2. What is the base directory of the file path  '/some/path/*.fa'? Is it stored in the `baseDir` global variable, as described [here](https://www.nextflow.io/docs/latest/script.html#script-implicit-variables)?\r\n\r\nI am quite new to this, so thanks for the help.",
    "metadata": {
      "platform": "github",
      "source": "GitHub Discussions",
      "url": "https://github.com/nextflow-io/nextflow/discussions/1950",
      "title": "Using file paths",
      "created_at": "2021-03-02T21:20:41Z"
    }
  },
  {
    "text": "## Bug report \r\n\r\nHi, \r\nI wrote a nextflow script that takes input data and does something with the data in several dockerized processes.\r\nIn a final step, I want to merge the results of different processes into one process that creates a final output.\r\nNextflow runs in parallel for several data samples, each with a different ID.\r\n\r\n\r\n\r\n### Expected behavior and actual behavior\r\n\r\nThe final merged results should come from data with the same ID, since it was the only input for the individual nextflow run.\r\n\r\nActually it seems that nextflow mixes up data from other runs that are performed in parallel.\r\nIn the work directory, I also find folders with different IDs in it.\r\n\r\nExample:\r\nInput data ID-1 --> processing --> result comes from ID1 and ID2\r\nInput data ID-2 --> processing --> result comes from ID2 and ID1\r\nInput data ID-3 --> processing --> result comes from ID3\r\n\r\n\r\n### Steps to reproduce the problem\r\n\r\nJust run the script again, but the IDs that are mixed up change each time. \r\n\r\n\r\n### Environment \r\n\r\n* Nextflow version: [19.10.0.5169 and 20.10.0.54.31] \r\n* Java version: [openjdk 11.0.10]\r\n* Operating system: [Ubuntu]\r\n* Bash version: (GNU bash version 4.4.20(1))",
    "metadata": {
      "platform": "github",
      "source": "GitHub Discussions",
      "url": "https://github.com/nextflow-io/nextflow/discussions/1954",
      "title": "is nextflow mixing up channel data when run in parallel?",
      "created_at": "2021-03-05T14:29:33Z"
    }
  },
  {
    "text": "I am seeing that after an error of one task other parallel executed tasks running on slurm are aborted even though I specified `errorStrategy = 'finish'` for the given process.\r\nIs this expected?\r\n\r\nAlso a related question. I would like my errorStrategy to be that all tasks that do not depend on the failed task are executed and run to completion. I have raid the [documentation](https://www.nextflow.io/docs/latest/process.html#errorstrategy) and none of the implemented strategies seems to do exactly that.\r\nIs there a way?\r\n\r\nThanks!",
    "metadata": {
      "platform": "github",
      "source": "GitHub Discussions",
      "url": "https://github.com/nextflow-io/nextflow/discussions/1959",
      "title": "Seeing Aborted Processe with errorStrategy = 'finish'",
      "created_at": "2021-03-09T00:39:24Z"
    }
  },
  {
    "text": "The weblog message contains the \"modules\" field under the \"metadata:parameters\" field when a pipeline written in DSL2 is started. The \"modules\" field lists process names without subworkflow names, and process names in the scope of the main workflow are not shown unexpectedly. However, the log message shown in the shell when launching a pipeline is correct.\r\nHow is this issue solved?",
    "metadata": {
      "platform": "github",
      "source": "GitHub Discussions",
      "url": "https://github.com/nextflow-io/nextflow/discussions/1960",
      "title": "the modules field in the weblog message for a started event in a DSL2 workflow",
      "created_at": "2021-03-09T08:30:48Z"
    }
  },
  {
    "text": "Hi Paolo,\r\n\r\nI was wondering that with the development of `plugins` in NF, would the roadmap include the possiblity of having different versions of same plugin (or Java lib) with the plugin/lib dependency tree. Is it similar to the way java handles it's deps?\r\n\r\nFor example\r\n\r\n```\r\nnf-plugin-1\r\n  |\r\n  |\r\n  _ nf-amazon@1.0.0\r\n\r\nnf-amazon@1.0.3\r\n```\r\n\r\nOr the above situation would never arise since all `nf plugins` are supposed to be designed in an independent manner instead of nesting.",
    "metadata": {
      "platform": "github",
      "source": "GitHub Discussions",
      "url": "https://github.com/nextflow-io/nextflow/discussions/1968",
      "title": "Is it possible to use plugins which depend on other plugins?",
      "created_at": "2021-03-12T16:27:38Z"
    }
  },
  {
    "text": "`join` seems to drop duplicate keys, and gives inconsistent results when used with `remainder: true`.\r\nThere's a similar (stale) [issue](https://github.com/nextflow-io/nextflow/issues/654) which I don't think got to the core of the problem, which *is not* about outer vs inner join.\r\n\r\nGiven this code:\r\n```\r\nleft = Channel.from(\r\n  ['k1', 'l1'],\r\n  ['k1', 'l2'],\r\n  ['k2', 'l3']\r\n  )\r\nright = Channel.from(\r\n  ['k1', 'r1'],\r\n  ['k2', 'r2']\r\n  )\r\n\r\nleft.join(right).view()\r\n```\r\nI get this output:\r\n```\r\n[k1, l1, r1]\r\n[k2, l3, r2]\r\n```\r\nwhile I would have expected this:\r\n```\r\n[k1, l1, r1]\r\n[k1, l2, r1]\r\n[k2, l3, r2]\r\n```\r\n\r\nThe result of using 'remainder: true' is even more puzzling. This code:\r\n```\r\nleft = Channel.from(['k1', 'l1'], ['k1', 'l2'], ['k2', 'l3'], ['k3', 'l4'])\r\nright = Channel.from(['k1', 'r1'], ['k2', 'r2'])\r\n\r\nleft.join(right, remainder: true).view()\r\n```\r\nresults into this:\r\n```\r\n[k1, l1, r1]\r\n[k2, l3, r2]\r\n[k1, l2, null]\r\n[k3, l4, null]\r\n```\r\n\r\nThe same behavior is shown by the 'cross' operator.",
    "metadata": {
      "platform": "github",
      "source": "GitHub Discussions",
      "url": "https://github.com/nextflow-io/nextflow/discussions/1975",
      "title": "Channel join behaving in surprising (incorrect?) ways",
      "created_at": "2021-03-16T09:40:59Z"
    }
  },
  {
    "text": "Hi, I'm creating a workflow in nextflow DSL2. I'm preparing a series of ready-to-use configurations, both for local and for our cluster computing system.\r\nMy problem is this: the local configuration uses a global memory, whereas the cluster uses a by-core memory. I've managed to configure it for the submission, so no problem on that side. My problem is, I'm using a software in which it is possible to specify the total memory usage, like:\r\n```\r\nMySoftware -maxmem ${ task.memory.giga }\r\n```  \r\n\r\nThis works fine for the local or standard profile, which uses a global memory set. However, of the cluster it should be more like:\r\n```\r\nMySoftware -maxmem ${ task.memory.giga * task.cpus }\r\n```\r\n\r\nIs there a way to change the behaviour based on the profile defined by the user? \r\nSomething like:\r\n```\r\nscripts:\r\nif (profile == cluster)\r\n    \"\"\"\r\n    MySoftware -maxmem ${ task.memory.giga * task.cpus }\r\n    \"\"\"\r\nelse \r\n    \"\"\"\r\n    MySoftware -maxmem ${ task.memory.giga }\r\n    \"\"\"\r\n```\r\nThanks for the help,\r\nAndrea",
    "metadata": {
      "platform": "github",
      "source": "GitHub Discussions",
      "url": "https://github.com/nextflow-io/nextflow/discussions/1976",
      "title": "Preferential script based on profile",
      "created_at": "2021-03-17T11:46:46Z"
    }
  },
  {
    "text": "[nextflow.txt](https://github.com/nextflow-io/nextflow/files/6174082/nextflow.txt)\r\nHi,\r\nI was trying to develop a pipeline from scratch using plink on nextflow. In this I have multiple input in each process and multiple outputs. How could I exactly define output variables (t2 in the script) which can be used in the next process? I was trying as below and getting attached message and output (nextflow.txt). Any help? Also is there any any tutorial/training of using plink in nextflow? I tried to follow https://seqera.io/training/#_multiple_output_files but it does not describe multiple outputs which can be used in the next process.\r\n```\r\nparams.dir='data/'\r\nparams.publish=\"output/\"\r\nChannel\r\n.fromFilePairs(\"${params.dir}*.{bed,bim,fam}\",size:3){file -> file.baseName}\r\n.set {data}\r\n\r\nprocess test1 {\r\npublishDir params.publish\r\ninput:\r\nset t1, file(bed), file(bim), file(fam) from data\r\noutput:\r\nset file(\"${t2}.bed\"), file(\"${t2}.bim\"), file(\"${t2}.fam\") into test1_results\r\nscript:\r\n\"\"\"\r\nmodule load plink\r\nplink --bfile ${t1} --maf 0.05 --make-bed --out t2\r\n\"\"\"\r\n}\r\nprocess test2 {\r\npublishDir params.publish\r\ninput:\r\nset t2, file(bed), file(bim), file(fam) from test1_results.collect()\r\noutput:\r\nset file(\"${t3}.bed\"), file(\"${t3}.bim\"), file(\"${t3}.fam\") into test2_results\r\nscript:\r\n\"\"\"\r\nplink --bfile ${t2} --maf 0.01 --make-bed --out t3\r\n\"\"\"\r\n}\r\n```",
    "metadata": {
      "platform": "github",
      "source": "GitHub Discussions",
      "url": "https://github.com/nextflow-io/nextflow/discussions/1981",
      "title": "Define output variable for multiple outputs of a process to use in the next process",
      "created_at": "2021-03-19T21:16:45Z"
    }
  },
  {
    "text": "Hello there! I'm studying nextflow, specially with  the DSL2 and I'm trying to name an optional output, my code is here:\r\n```\r\nnextflow.enable.dsl=2\r\n\r\nprocess foo{\r\n\r\n\toutput: \r\n\ttuple val(a), file(\"*.txt\")\r\n\ttuple val(b), file(\"*.tsv\") optional true, emit: test_channel\r\n\ttuple val(c), file(\"*.csv\")\r\n\tscript:\r\n\ta = \"1\"\r\n\tb = \"2\"\r\n\tc = \"3\"\r\n\t\"\"\"\r\n\ttouch nothing.txt\r\n\ttouch nothing.csv\r\n\t\"\"\"\r\n}\r\n\r\nworkflow{\r\n foo()\r\n}\r\n```\r\n\r\nI'm trying to emit and optional output and nextflow returns me this error: \r\n```\r\nN E X T F L O W  ~  version 21.03.0-edge\r\nLaunching `test.nf` [crazy_minsky] - revision: 935e2b6a7d\r\nNo such variable: test_channel\r\n```\r\nThis feature exists? or should I request it?\r\n\r\nThanks for your attention.",
    "metadata": {
      "platform": "github",
      "source": "GitHub Discussions",
      "url": "https://github.com/nextflow-io/nextflow/discussions/1980",
      "title": "DSL2: Can you emit:  an optional output?",
      "created_at": "2021-03-20T20:15:08Z"
    }
  },
  {
    "text": "I have a script which frequently gives ConcurrentModificationException errors soon after starting.\r\n\r\nI think it will be very hard to provide anything near to a reproducible example! My script uses extensive caching:\r\n\r\nWe reach the stage where it looks like two jobs have been submitted to PBS.\r\n\r\n```\r\nexecutor >  local (1), pbspro (2)\r\n[f2/39ba5c] process > removeDuplicates        [100%] 1 of 1 âœ”\r\n[-        ] process > step_2_trim_quality     -\r\n[-        ] process > step_2_code             -\r\n[skipped  ] process > step_3_kraken_batch (3) [100%] 5 of 5, stored: 5 âœ”\r\n[skipped  ] process > step_3_code             [100%] 1 of 1, stored: 1 âœ”\r\n[skipped  ] process > step_4_hg_alignment     [100%] 1 of 1, stored: 1 âœ”\r\n[-        ] process > step_4_code             -\r\n[-        ] process > step_5_panphlan         -\r\n[-        ] process > step_5_code             -\r\n[ba/633101] process > step_6_kraken_batch (4) [100%] 3 of 3, stored: 3\r\n```\r\n\r\nActor Thread 8 crashed, but I can't tell what it was doing at the point it crashed.\r\n```\r\nMar-26 12:11:02.793 [Actor Thread 8] ERROR nextflow.extension.DataflowHelper - @unknown\r\njava.util.ConcurrentModificationException: null\r\n        at java.base/java.util.ArrayList$Itr.checkForComodification(ArrayList.java:1043)\r\n        at java.base/java.util.ArrayList$Itr.next(ArrayList.java:997)\r\n        at org.codehaus.groovy.runtime.DefaultGroovyMethods.each(DefaultGroovyMethods.java:2329)\r\n        at org.codehaus.groovy.runtime.DefaultGroovyMethods.each(DefaultGroovyMethods.java:2315)\r\n        at org.codehaus.groovy.runtime.DefaultGroovyMethods.each(DefaultGroovyMethods.java:2356)\r\n        at org.codehaus.groovy.runtime.dgm$186.invoke(Unknown Source)\r\n        at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite$PojoMetaMethodSiteNoUnwrapNoCoerce.invoke(PojoMetaMethodSite.java:244)\r\n        at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite.call(PojoMetaMethodSite.java:53)\r\n        at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:127)\r\n        at nextflow.extension.OperatorEx$_flatMap_closure2.doCall(OperatorEx.groovy:194)\r\n        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n        at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:101)\r\n        at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:323)\r\n        at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:263)\r\n        at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1041)\r\n        at groovy.lang.Closure.call(Closure.java:405)\r\n        at groovyx.gpars.dataflow.operator.DataflowOperatorActor.startTask(DataflowOperatorActor.java:120)\r\n        at groovyx.gpars.dataflow.operator.DataflowOperatorActor.onMessage(DataflowOperatorActor.java:108)\r\n        at groovyx.gpars.actor.impl.SDAClosure$1.call(SDAClosure.java:43)\r\n        at groovyx.gpars.actor.AbstractLoopingActor.runEnhancedWithoutRepliesOnMessages(AbstractLoopingActor.java:293)\r\n        at groovyx.gpars.actor.AbstractLoopingActor.access$400(AbstractLoopingActor.java:30)\r\n        at groovyx.gpars.actor.AbstractLoopingActor$1.handleMessage(AbstractLoopingActor.java:93)\r\n        at groovyx.gpars.util.AsyncMessagingCore.run(AsyncMessagingCore.java:132)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n        at java.base/java.lang.Thread.run(Thread.java:834)\r\nMar-26 12:11:02.799 [Actor Thread 8] DEBUG nextflow.Session - Session aborted -- Cause: java.util.ConcurrentModificationException\r\nMar-26 12:11:02.827 [Actor Thread 8] DEBUG nextflow.Session - The following nodes are still active:\r\n[process] step_4_code\r\n  status=ACTIVE\r\n  port 0: (value) bound ; channel: version\r\n  port 1: (value) bound ; channel: prior_versions\r\n  port 2: (value) bound ; channel: code\r\n  port 3: (value) bound ; channel: prior_code\r\n  port 4: (value) bound ; channel: script_name\r\n  port 5: (cntrl) -     ; channel: $\r\n\r\n[process] step_5_panphlan\r\n  status=ACTIVE\r\n  port 0: (value) bound ; channel: files_in\r\n  port 1: (value) bound ; channel: version\r\n  port 2: (value) bound ; channel: prior_versions\r\n  port 3: (value) bound ; channel: script_name\r\n  port 4: (queue) closed; channel: organism\r\n  port 5: (cntrl) -     ; channel: $\r\n\r\n[process] step_5_code\r\n  status=ACTIVE\r\n  port 0: (value) OPEN  ; channel: version\r\n  port 1: (value) bound ; channel: prior_versions\r\n  port 2: (value) bound ; channel: code\r\n  port 3: (value) OPEN  ; channel: prior_code\r\n  port 4: (value) bound ; channel: script_name\r\n  port 5: (cntrl) -     ; channel: $\r\n\r\nMar-26 12:11:02.829 [main] DEBUG nextflow.Session - Session await > all process finished\r\nMar-26 12:11:02.832 [main] DEBUG nextflow.Session - Session await > all barriers passed\r\nMar-26 12:11:02.845 [main] WARN  n.processor.TaskPollingMonitor - Killing pending tasks (2)\r\nMar-26 12:11:03.527 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=1; failedCount=0; ignoredCount=0; cachedCount=0; pendingCount=1; submittedCount=2; runningCount=-2; retriesCount=0; abortedCount=2; succeedDuration=42ms; failedDuration=0ms; cachedDuration=0ms;loadCpus=-40; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=10 MB; ]\r\nMar-26 12:11:04.298 [main] DEBUG nextflow.CacheDB - Closing CacheDB done\r\nMar-26 12:11:04.528 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye\r\n```\r\n\r\nMy script is long but ultimately not that complex. It is compiled from separate files which I change track, and outputs its own code with the processed data, which is the main novelty. It worked no problem for a long time, and I'm not sure what it is about the inputs or processing which is leading to this error.\r\n\r\nI'd be very grateful for any pointers for chasing down ConcurrentModificationExceptions.\r\n\r\n[main.txt](https://github.com/nextflow-io/nextflow/files/6211826/main.txt)\r\n\r\nThanks,\r\n\r\nAndrew",
    "metadata": {
      "platform": "github",
      "source": "GitHub Discussions",
      "url": "https://github.com/nextflow-io/nextflow/discussions/1994",
      "title": "ConcurrentModificationException",
      "created_at": "2021-03-26T12:41:08Z"
    }
  },
  {
    "text": "I am using Nextflow 21.03.0-edge from a Linux cluster on my campus, and am trying to test out the [Azure Batch](https://www.nextflow.io/docs/edge/azure.html) functionality so I can write up documentation for a project I'm on.  I'm getting the error:\r\n\r\n```\r\nWARN: Nextflow self-contained distribution only allows default plugins -- User config plugins will be ignored: nf-azure\r\nCannot a find a file system provider for scheme: az\r\n```\r\n\r\nI assume that there is something we need to change about the way Nextflow is installed on our cluster in order to allow it to use plugins.  I haven't been able to find any documentation on this topic.",
    "metadata": {
      "platform": "github",
      "source": "GitHub Discussions",
      "url": "https://github.com/nextflow-io/nextflow/discussions/1995",
      "title": "Warning when trying to use a plugin",
      "created_at": "2021-03-26T13:48:50Z"
    }
  }
]